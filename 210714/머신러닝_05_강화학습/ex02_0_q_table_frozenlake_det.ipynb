{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FrozenLake 게임 환경 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "import random\n",
    "\n",
    "#pip install gym\n",
    "\n",
    "#최대값이 모두 같을 때 random하게 return하도록 작성한 코드.  \n",
    "#최대값이 있으면 걔를 return함\n",
    "def rargmax(vector):     \n",
    "    m = np.max(vector)\n",
    "    indices = np.nonzero(vector == m)[0]\n",
    "    return random.choice(indices)\n",
    "\n",
    "register(  #게임설정 (게임이름을 id로 따로 다시 지정 등 환경설정)\n",
    "    id='FrozenLake-v3',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False}\n",
    ")\n",
    "env = gym.make ('FrozenLake-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q table 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q를 모두 0으로 초기화.  Q[16,4]\n",
    "# Q테이블 만들기 위해 비슷하게만들려고 16행 4개열로 만들고 1행이 Q테이블의 state1개의미\n",
    "# 행안의 열들이 왼쪽,오른쪽,아래,위의 값 가짐\n",
    "#np.zeros(현재 환경의 상태의수, 액션의 수) =16,4\n",
    "Q = np.zeros ([env.observation_space.n, env.action_space.n]) \n",
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 강화 학습 (Q learning을 통한) \n",
    "학습을 하면서 게임을 하므로 게임 초반에는 실패가 많으나 후반으로 갈수록 성공 확률이 높아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#강화학습은 실제로 환경만들기가 제일 어렵다. 우리는 만들어진 환경 가져오는거\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "rList = []\n",
    "for i in range(num_episodes): # 여러번 반복 학습 (2000번 게임함)\n",
    "    state =  env.reset()  # 환경 reset 후, 첫번째 상태 얻음 \n",
    "    rAll = 0\n",
    "    done = False\n",
    "\n",
    "    # The Q-Table learning algorithm\n",
    "    while not done: #게임 끝날때까지 반복\n",
    "        \n",
    "        #현재 state의 Q중 최대 reward를 얻을 수 있는 action을 구함. \n",
    "        #다 0이면 random하게 선택, 만약 1이 생기면 2가 생긴 action값을 취함\n",
    "        action = rargmax(Q[state,:])\n",
    "\n",
    "        # 환경에서 action 후, new_state와 reward를 얻음\n",
    "        # action ( 0 - left, 1 -douwn, 2-right, 3-up )\n",
    "        new_state,reward,done, _= env.step(action)\n",
    "        \n",
    "        # Q-Table 갱신 (새로 얻은 상태정보 같은거 업뎃)\n",
    "        #reward와 새로갱신한 상태의 최대값\n",
    "        Q[state,action]=reward+np.max(Q[new_state,:]) \n",
    "\n",
    "        rAll += reward #reward합 받음. 최대값은 1일수밖에없음\n",
    "        state = new_state\n",
    "    rList.append(rAll) #rList가 2000번 게임이 각각끝났을때 0이나 1로 끝났는지 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 게임 결과 출력\n",
    "(학습을 하면서 게임을 하므로 게임 초반에는 실패가 많으나 후반으로 갈수록 성공 확률이 높아진다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.946\n",
      "Final Q-Table Values\n",
      "LEFT DOWN RIGHT UP\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPGElEQVR4nO3dfaycaV3G8e9lyxKRlwX2QNa+0GIK2j94WeqCURCDQrtRKkpMF8LiCmk2oQZiTLaGBEn4RyQYQ1hoKjYLBCkxLFJJYTFEIQZWtov7VpYuh/Kyh667XTBAxLgWfv4xT3E6zJmXdmbO9s73k5ycee7nnpkr98xefeaZM7OpKiRJF7+fWesAkqTZsNAlqREWuiQ1wkKXpEZY6JLUiPVrdceXXXZZbdmyZa3uXpIuSrfddttDVbU0bN+aFfqWLVs4duzYWt29JF2UknxztX2ecpGkRljoktQIC12SGmGhS1IjLHRJaoSFLkmNGFvoSQ4leTDJ3avsT5J3JVlOcmeSK2YfU5I0ziRH6DcCO0fs3wVs6372Au+98FiSpGmNLfSq+hzw3RFTdgMfqJ5bgEuTXD6rgJKkyczik6IbgPv6tle6sfsHJybZS+8ons2bN8/grs/PwYO933v39i6f/T2M++azb9icFvZNswbum37fI+ExnsW+eZnFm6IZMjb0f4NUVQerakdV7VhaGvpVBJKk8zSLQl8BNvVtbwROzeB2JUlTmEWhHwGu6f7a5QXA96rqp063SJLma+w59CQfBl4MXJZkBfhz4FEAVXUAOApcBSwDPwSunVdYSdLqxhZ6VV09Zn8Bb5hZIknSefGTopLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGTFToSXYmOZFkOcn+IfufkOQfk9yR5HiSa2cfVZI0ythCT7IOuAHYBWwHrk6yfWDaG4AvV9WzgRcD70xyyYyzSpJGmOQI/UpguapOVtXDwGFg98CcAh6XJMBjge8CZ2aaVJI00iSFvgG4r297pRvr927gl4BTwF3AG6vqx4M3lGRvkmNJjp0+ffo8I0uShpmk0DNkrAa2XwbcDvw88Bzg3Uke/1NXqjpYVTuqasfS0tKUUSVJo0xS6CvApr7tjfSOxPtdC9xUPcvA14FfnE1ESdIkJin0W4FtSbZ2b3TuAY4MzPkW8BKAJE8FngmcnGVQSdJo68dNqKozSfYBNwPrgENVdTzJdd3+A8DbgBuT3EXvFM31VfXQHHNLkgaMLXSAqjoKHB0YO9B3+RTw0tlGkyRNw0+KSlIjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhoxUaEn2ZnkRJLlJPtXmfPiJLcnOZ7ks7ONKUkaZ/24CUnWATcAvwWsALcmOVJVX+6bcynwHmBnVX0ryVPmlFeStIpJjtCvBJar6mRVPQwcBnYPzHkVcFNVfQugqh6cbUxJ0jiTFPoG4L6+7ZVurN8zgCcm+ZcktyW5ZlYBJUmTGXvKBciQsRpyO88DXgL8LPCFJLdU1b3n3FCyF9gLsHnz5unTSpJWNckR+gqwqW97I3BqyJxPVdV/VdVDwOeAZw/eUFUdrKodVbVjaWnpfDNLkoaYpNBvBbYl2ZrkEmAPcGRgzseBFyZZn+QxwPOBe2YbVZI0ythTLlV1Jsk+4GZgHXCoqo4nua7bf6Cq7knyKeBO4MfA+6rq7nkGlySda5Jz6FTVUeDowNiBge13AO+YXTRJ0jT8pKgkNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDViokJPsjPJiSTLSfaPmPfLSX6U5JWziyhJmsTYQk+yDrgB2AVsB65Osn2VeW8Hbp51SEnSeJMcoV8JLFfVyap6GDgM7B4y74+BjwIPzjCfJGlCkxT6BuC+vu2VbuwnkmwAXgEcGHVDSfYmOZbk2OnTp6fNKkkaYZJCz5CxGtj+a+D6qvrRqBuqqoNVtaOqdiwtLU0YUZI0ifUTzFkBNvVtbwRODczZARxOAnAZcFWSM1X1D7MIKUkab5JCvxXYlmQr8G1gD/Cq/glVtfXs5SQ3Ap+wzCVpscYWelWdSbKP3l+vrAMOVdXxJNd1+0eeN5ckLcYkR+hU1VHg6MDY0CKvqj+88FiSpGn5SVFJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyYq9CQ7k5xIspxk/5D9r05yZ/fz+STPnn1USdIoYws9yTrgBmAXsB24Osn2gWlfB369qp4FvA04OOugkqTRJjlCvxJYrqqTVfUwcBjY3T+hqj5fVf/Zbd4CbJxtTEnSOJMU+gbgvr7tlW5sNa8DPjlsR5K9SY4lOXb69OnJU0qSxpqk0DNkrIZOTH6DXqFfP2x/VR2sqh1VtWNpaWnylJKksdZPMGcF2NS3vRE4NTgpybOA9wG7quo7s4knSZrUJEfotwLbkmxNcgmwBzjSPyHJZuAm4DVVde/sY0qSxhl7hF5VZ5LsA24G1gGHqup4kuu6/QeAtwBPBt6TBOBMVe2YX2xJ0qBJTrlQVUeBowNjB/ouvx54/WyjSZKm4SdFJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERMVepKdSU4kWU6yf8j+JHlXt//OJFfMPqokaZSxhZ5kHXADsAvYDlydZPvAtF3Atu5nL/DeGeeUJI0xyRH6lcByVZ2sqoeBw8DugTm7gQ9Uzy3ApUkun3FWSdIIqarRE5JXAjur6vXd9muA51fVvr45nwD+oqr+tdv+DHB9VR0buK299I7gAZ4JnDjP3JcBD53ndeftkZrNXNMx13TMNZ0LyfW0qloatmP9BFfOkLHBfwUmmUNVHQQOTnCfowMlx6pqx4Xezjw8UrOZazrmmo65pjOvXJOcclkBNvVtbwROncccSdIcTVLotwLbkmxNcgmwBzgyMOcIcE331y4vAL5XVffPOKskaYSxp1yq6kySfcDNwDrgUFUdT3Jdt/8AcBS4ClgGfghcO7/IwAxO28zRIzWbuaZjrumYazpzyTX2TVFJ0sXBT4pKUiMsdElqxEVX6OO+hmDO970pyT8nuSfJ8SRv7MbfmuTbSW7vfq7qu86fdVlPJHnZHLN9I8ld3f0f68aelOSfkny1+/3EReZK8sy+Nbk9yfeTvGkt1ivJoSQPJrm7b2zq9UnyvG6dl7uvuxj2J7sXmusdSb7SfY3Gx5Jc2o1vSfLffet2YMG5pn7cFpTrI32ZvpHk9m58keu1Wjcs9jlWVRfND703Zb8GPB24BLgD2L7A+78cuKK7/DjgXnpfh/BW4E+HzN/eZXw0sLXLvm5O2b4BXDYw9pfA/u7yfuDti8418Nj9B/C0tVgv4EXAFcDdF7I+wBeBX6H32YtPArvmkOulwPru8tv7cm3pnzdwO4vINfXjtohcA/vfCbxlDdZrtW5Y6HPsYjtCn+RrCOamqu6vqi91l38A3ANsGHGV3cDhqvqfqvo6vb8CunL+Sc+5//d3l98P/O4a5noJ8LWq+uaIOXPLVVWfA7475P4mXp/0vs7i8VX1her9l/eBvuvMLFdVfbqqznSbt9D7XMeqFpVrhDVdr7O6I9k/AD486jbmlGu1bljoc+xiK/QNwH192yuMLtS5SbIFeC7wb93Qvu4l8qG+l1WLzFvAp5Pclt5XLAA8tbrPA3S/n7IGuc7aw7n/oa31esH067Ohu7yofAB/RO8o7aytSf49yWeTvLAbW2SuaR63Ra/XC4EHquqrfWMLX6+Bbljoc+xiK/SJvmJg7iGSxwIfBd5UVd+n9+2SvwA8B7if3ss+WGzeX62qK+h98+UbkrxoxNyFrmN6H0h7OfD33dAjYb1GWS3HotftzcAZ4EPd0P3A5qp6LvAnwN8lefwCc037uC368byacw8aFr5eQ7ph1amrZLigbBdboa/5VwwkeRS9B+xDVXUTQFU9UFU/qqofA3/D/58mWFjeqjrV/X4Q+FiX4YHuJdzZl5kPLjpXZxfwpap6oMu45uvVmXZ9Vjj39Mfc8iV5LfDbwKu7l950L8+/012+jd5512csKtd5PG6LXK/1wO8BH+nLu9D1GtYNLPg5drEV+iRfQzA33Tm6vwXuqaq/6hvv/6rgVwBn34E/AuxJ8ugkW+l9X/wX55Dr55I87uxlem+q3d3d/2u7aa8FPr7IXH3OOXJa6/XqM9X6dC+Zf5DkBd1z4Zq+68xMkp3A9cDLq+qHfeNL6f3/CUjy9C7XyQXmmupxW1Suzm8CX6mqn5yuWOR6rdYNLPo5diHv7K7FD72vGLiX3r+2b17wff8avZc/dwK3dz9XAR8E7urGjwCX913nzV3WE1zgO+kjcj2d3jvmdwDHz64L8GTgM8BXu99PWmSu7n4eA3wHeELf2MLXi94/KPcD/0vvKOh157M+wA56RfY14N10n7aeca5leudXzz7HDnRzf797fO8AvgT8zoJzTf24LSJXN34jcN3A3EWu12rdsNDnmB/9l6RGXGynXCRJq7DQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiP+D2aBSg0N3vyyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Success rate: \" + str(sum(rList) / num_episodes)) #게임에서얻은 reward/게임횟수\n",
    "print(\"Final Q-Table Values\")\n",
    "print(\"LEFT DOWN RIGHT UP\")\n",
    "print(Q)\n",
    "\n",
    "#rList 그림 그려보면 초반에는 0이많지만 뒤로갈수록 길을 알게되기때문에 1이 많아짐\n",
    "plt.bar(range(len(rList)), rList, color=\"b\", alpha=0.4) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단점: 길이 한번만들어지면 계속 그 길로만 안내한다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
